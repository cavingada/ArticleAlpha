{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from datetime import datetime as dt\n",
    "import pytz\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Credit to / help from https://saturncloud.io/blog/how-to-remove-stop-words-from-a-pandas-dataframe-using-python/\n",
    "def remove_stopwords(words_tokenized):\n",
    "    stop_words = set(stopwords.words('english'))  # List of english stopwords\n",
    "    return [word for word in words_tokenized if word not in stop_words] # Using list comprehension, only choose the words that aren't stopwords\n",
    "\n",
    "def convert_to_datetime(date_string):\n",
    "    \n",
    "    date_string, _ = date_string.rsplit(\"-\", 1)\n",
    "    \n",
    "    converted_date = dt.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n",
    "    return converted_date\n",
    "\n",
    "def preprocess(df, sample_size=None):\n",
    "    df = df.dropna()\n",
    "    # sample if specified\n",
    "    if sample_size:\n",
    "        df = df.sample(sample_size)\n",
    "    # remove uncessary index column\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    # change stock column name to ticker\n",
    "    df.rename(columns={'stock': 'ticker'}, inplace=True)\n",
    "    # convert headlines to lowercase\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    # remove punctuation\n",
    "    df['title'] = df['title'].str.replace(r'[^a-zA-Z\\s$0-9]', '', regex=True)\n",
    "    # tokenize\n",
    "    df['title'] = df['title'].str.split() \n",
    "    # remove stopwords\n",
    "    df['title'] = df['title'].apply(remove_stopwords)\n",
    "    # convert to datetime object\n",
    "    df['date'] = df['date'].apply(convert_to_datetime)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab Stock Returns\n",
    "\n",
    "Based on the time of the article published, we will retrieve two adjusted close prices of the stock and compute the corresponding return.\n",
    "\n",
    "If the time of the article is published before 4:00 P.M. (non-inclusive), then:\n",
    "1. The 'before' price will be the most recent (before the date) trading day's adjusted close price\n",
    "2. The 'after' price will be the most upcoming trading day's adjusted close price\n",
    "\n",
    "If the time of the article is published after 4:00 P.M., then:\n",
    "1. The 'before' price will be the same day's adjusted close price\n",
    "2. The 'after' priec will be the next day's adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_market_calendars as mcal\n",
    "from datetime import timedelta\n",
    "# The paramater forward is a boolean representing whether we are looking for the next valid trading day or the most recent trading day\n",
    "def getValidTradingCloseDate(date, forward=True):\n",
    "        nyse = mcal.get_calendar('NYSE')\n",
    "        if forward:\n",
    "            start_date = date\n",
    "            end_date = date+timedelta(days=15)\n",
    "        else:\n",
    "            start_date = date-timedelta(days=15)\n",
    "            end_date = date\n",
    "\n",
    "        validTradingDays = nyse.valid_days(start_date=start_date , end_date=end_date)\n",
    "        return validTradingDays.date[2] if forward else validTradingDays.date[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the yfinance data we need based on date.\n",
    "import yfinance as yf\n",
    "\n",
    "def retrieve_yfinance_data(row):\n",
    "    curr_date = row['date']\n",
    "    \n",
    "    eod = dt.strptime('16:00:00', '%H:%M:%S').time()\n",
    "    \n",
    "    if curr_date.time() > eod:\n",
    "        start_date = curr_date.date()\n",
    "        end_date = getValidTradingCloseDate(start_date, forward=True)\n",
    "    else:\n",
    "        end_date = curr_date.date()\n",
    "        start_date = getValidTradingCloseDate(end_date, forward=False)\n",
    "        end_date = end_date + timedelta(days=1)\n",
    "        \n",
    "    data = yf.download(row['ticker'], start=start_date, end=end_date, progress=False, show_errors=False)\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        returns = (data['Adj Close'][-1] - data['Adj Close'][0]) / data['Adj Close'][0]\n",
    "        return returns\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_returns(df):\n",
    "    # df['returns'] = df.apply(retrieve_yfinance_data, axis=1)\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        returns = retrieve_yfinance_data(row)\n",
    "        # print(returns)\n",
    "        df.loc[idx,'returns'] = returns\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment this code to \n",
    "1. sample two datasets of 50k samples from the 800k+ samples from the dataset.\n",
    "2. save each dataset to its own pkl file \n",
    "\n",
    "*Note: We saved 2 datasets of 50k samples each because we had two people download them simultaneously to save time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifty_thousand_articles_1 = preprocess(articles, sample_size=50000)\n",
    "# fifty_thousand_articles_1 = get_returns(fifty_thousand_articles_1)\n",
    "# pd.to_pickle(fifty_thousand_articles_1, '50k_processed_articles_1.pkl')\n",
    "\n",
    "# fifty_thousand_articles_2 = preprocess(articles, sample_size=50000)\n",
    "# fifty_thousand_articles_2 = get_returns(50k_articles_2)\n",
    "# pd.to_pickle(fifty_thousand_articles_2, '50k_processed_articles_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Two Datasets into 1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both datasets\n",
    "dataset_1 = pd.read_pickle('50k_processed_data_1.pkl')\n",
    "dataset_2 = pd.read_pickle('50k_processed_data_2.pkl')\n",
    "\n",
    "# merge the two datasets (disclude repeated rows)\n",
    "dataset = pd.concat([dataset_1, dataset_2], axis=0)\n",
    "\n",
    "# eliminate rows with the same index\n",
    "dataset = dataset[~dataset.index.duplicated(keep='first')]\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataset into Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split(df):\n",
    "    x_train, x_test_and_val, y_train, y_test_and_val  = train_test_split(df['title'], df['returns'], random_state=42, test_size=0.2) # train set is 80%,\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test_and_val, y_test_and_val, test_size=0.5, random_state=42) # test and val are 50% of the remaining 20% = 10%. \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split(dataset) # split the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(dataset, 'Data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the articles dataset distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_returns(ax, datasetType, returns, bins=None):\n",
    "    # If bins isn't specified, set it to 1/10th of the number of returns\n",
    "\n",
    "    ax.hist(returns, bins=bins, alpha=0.7, color='b', edgecolor='black')\n",
    "    ax.set_xlabel(f'{datasetType} Returns')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of {datasetType} Returns')\n",
    "    ax.set_xlim(-0.4, 0.4)\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean_return = np.mean(returns)\n",
    "    std_deviation = np.std(returns)\n",
    "\n",
    "    # Add mean and standard deviation to the plot\n",
    "    ax.text(0.05, 0.9, f'Mean: {mean_return:.4f}', transform=ax.transAxes)\n",
    "    ax.text(0.05, 0.85, f'Standard Deviation: {std_deviation:.4f}', transform=ax.transAxes)\n",
    "\n",
    "def display_return_plots(y_train, y_val, y_test):\n",
    "    # Create subplots\n",
    "    _, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plot train, val, and test returns\n",
    "    plot_returns(axs[0], 'Train', y_train, bins=2000)\n",
    "    plot_returns(axs[1], 'Validation', y_val, bins=3000)\n",
    "    plot_returns(axs[2], 'Test', y_test, bins=3000)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_return_plots(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(dataset, 'Data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(x, y):\n",
    "    num_samples = len(x)\n",
    "    \n",
    "    num_pos = len(y[y > 0])\n",
    "    num_neg = len(y[y < 0])\n",
    "    num_zero = len(y[y == 0])\n",
    "\n",
    "    list_lengths = x.apply(len)\n",
    "\n",
    "    # Find the size of the smallest/largest list\n",
    "    num_min_tokens = min(list_lengths)\n",
    "    num_max_tokens = max(list_lengths)\n",
    "    num_mean_tokens = list_lengths.mean()\n",
    "    \n",
    "    return {\"Number of Samples\":num_samples, \n",
    "            \"Number of Samples with Positive Returns\": num_pos,\n",
    "            \"Number of Samples with No Returns\": num_zero,\n",
    "            \"Number of Samples with Negative Returns\": num_neg,\n",
    "            \"Minimum Number of Tokens\": num_min_tokens, \n",
    "            \"Maximum Number of Tokens\":num_max_tokens, \n",
    "            \"Mean Number of Tokens\":num_mean_tokens}\n",
    "\n",
    "def df_for_analysis(train_analysis, test_analysis, validation_analysis):\n",
    "    df = pd.DataFrame([train_analysis, test_analysis, validation_analysis], index=['Train', 'Test', 'Validation'])\n",
    "    return df\n",
    "\n",
    "analysis_df = df_for_analysis(analyze(x_train, y_train), analyze(x_test,y_test), analyze(x_val, y_val))\n",
    "\n",
    "# export dataset analysis dataframe as png\n",
    "import dataframe_image as dfi\n",
    "dfi.export(analysis_df, 'Data Analysis/Dataset Analysis.png')\n",
    "\n",
    "analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "articles = pd.read_csv('articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
